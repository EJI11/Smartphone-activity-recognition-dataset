***********A dataset for human activity recognition for wearable sensors**************

Date: 3/8/2025
Institution: Xi'an University of Architecture and Technology

NOTE: if you use this dataset please cite the following work

X. Geng, X. Chen, Z. Ma, L. Song, Multi-framework evidential association rule fusion for wearable human activity recognition, IEEE Sens.
J. 24 (2024) 11805–11816.


For collection, we invited 12 women and 12 men volunteers with variant heights (women are from 1.55 to 1.75 meters and men are from 1.7 to 1.85 meters) and weights (women are from 45 to 60 kilograms and men are from 60 to 85 kilograms) to perform six daily activities in our laboratory, with a smartphone (type of HUAWEI Nova8 Pro) carried in their right pockets of pants. The dataset contains the activities of walking, upstairs, downstairs, sitting, standing and lying. During the collection, the sensory signals for each single type of activity are collected at a time lasting 15 seconds, from three embedded sensors (accelerometer, gyroscope and orientation sensor) in the smartphone with a fixed sampling frequency of 50Hz. Thus, the raw sensory signals refer to triaxis acceleration, triaxis angular velocity, azimuth, pitch and roll signals. 



The activity set is listed in the following:

1-Walking
2-Upstairs
3-Downstairs
4-Sitting
5-Standing 
6-Lying



The first column of raw data is labeled, columns 2-4 are 3-axis acceleration data, columns 5-7 are 3-axis angular velocity data, and columns 8-13 are orientation sensor data.

The folder “All subject” holds the data of all subjects, and "SelfData.mat" stores all subjects into one cytosol.


